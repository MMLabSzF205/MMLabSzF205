---
title: 强化学习笔记
date: 2019-06-11 11:04:31
tags: 强化学习
categories: 严浩鹏
mathjax: true
---

#### 1. 马尔科夫过程

马尔可夫性：已知当前的状态，过去与未来无关。
公式表示：$P(S_{t+1} | S_{t}) = P(S_{t+1} | S_{t}, S_{t-1}, ..., S_{1})$

马尔可夫过程分类：
* 马尔可夫奖励过程（MRP）
   * 表示为{S, P, R, $\gamma$}
* 马尔可夫决策过程（MDP）
   * {S, A, P, R, $\gamma$} 

其中S是状态的集合， A是动作的集合， P是动作到状态的策略，V是状态的值函数，R则是执行一个动作从一个状态转移到另一个状态的奖惩函数，$\gamma$是衰减参数。

MDP可以分为全观测MDP和部分可观测MDP（POMDP）

#### 2. 强化学习的分类

* 按照是否有模型分类
    * Model Free
    * Model Based

* 按照是否有策略和值函数
    * Policy Based
        * Policy
        * No Value Function
    * Value Based
        * No Policy
        * Value Function
    * Actor Critic
        * Policy
        * Value Function

#### 3. 探索与利用

强化学习需要在探索与利用之间做出一定的权衡。强化学习Agent需要不断地探索，从环境中获取经验。与此同时，它也需要利用现有的经验决策以获取较高的回报。
Agent不能只是探索或者只利用。一味地探索会导致回报值过低，而一味地利用则可能陷入局部最优。


#### 4. 马尔科夫奖励过程（MRP）

表示为： {S, P, R, $\gamma$}

令$G_t$为打折后的奖励值：
$$G_t = R_{t+1} + \gamma R_{t+2} + ... = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}$$

衰减参数$\gamma \in [0, 1]$, $\gamma$越接近1表示未来的值得影响越大，接近表示未来的重要性越小。

值函数：
$$V(s) =  \mathbb{E} [ G_t | S_t = s ]$$

Bellman等式：
$$
\begin{aligned}
V(s) &= \mathbb{E}[G_t | S_t = s]\\
    &= \mathbb{E}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+2} + ... | S_t = s] \\
    &= \mathbb{E}[R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+2} + ...) | S_t = s] \\
    &= \mathbb{E}[R_{t+1} + \gamma V(S_{t+1}) | S_t = s]
\end{aligned}
$$

转化成： 
$$V(s) = \mathbb{E}[R_s + \gamma \sum_{s^{'} \in S} P_{ss'} v(s^{'})]$$

矩阵表示： 
$$v = R + \gamma P v$$

变换:
$$
\begin{aligned}
v &= R + \gamma P v \\
    (I - \gamma P) v &= R \\
    v &= (I - \gamma P)^{-1} R
\end{aligned}
$$

#### 5. 马尔科夫决策过程(MDP)

表示为: {S, A, P, R, $\gamma$}

MDP与MRP的区别：
* MRP

![02543b32372f78f1f98813717b9e2644](51C658D7-D7E4-41F0-BE21-68B5FDEB328B.png)
* MDP

![1ae07e08ae273f79dcc32e69c241c423](A5889761-76B1-41F9-BA85-D24667B330C9.png)
 
与MRP相比，MDP除了状态值函数，还增加了动作值函数。
* 状态值函数：
$$v_{\pi}(s) =  \mathbb{E} [ G_t | S_t = s]$$

* 动作值函数：
$$q_{\pi}(s, a) =  \mathbb{E} [ G_t | S_t = s, A_t = a ]$$

和状态值函数的推导相似，动作值函数可以化简为：

$$q_{\pi}(s, a) =  \mathbb{E} [ R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) | S_t = s,  A_t = a ]$$

状态值函数使用Q值函数表示：
$$v_{\pi}(s) =  \sum_{a \in A} \pi(a|s) q_{\pi}(s, a)$$

Q值函数表示：
$$q_{\pi}(s, a) = R_{s}^{a} + \gamma \sum_{s^{'} \in S} P_{ss^{'}}^a v_{\pi}(s^{'})$$

Q值函数代入状态值函数中：

$$v_{\pi}(s) =  \sum_{a \in A} \pi(a|s) \left (R_{s}^{a} + \gamma \sum_{s^{'} \in S} P_{ss^{'}}^a v_{\pi}(s^{'}) \right )$$

状态值函数代入Q值函数中：
$$q_{\pi}(s, a) = R_{s}^{a} + \gamma \sum_{s^{'} \in S} P_{ss^{'}}^a   \sum_{a^{'} \in A} \pi(a^{'}|s^{'}) q_{\pi}(s^{'}, a^{'})$$

矩阵表示： 
$$v^{\pi} = R^{\pi} + \gamma P^{\pi} v^{\pi}$$

变换:
$$v^{\pi} = (I - \gamma P{\pi})^{-1} R^{\pi}$$

寻找最优的值函数：
![14ef6bdb0d7e7aecd6c7044496d7ed51](1B69E559-0C65-4605-AD6A-D729C74AD155.png)
![6db938239e558d5a35c60becd7eaa19a](86F9B82B-1EE0-45AF-9B15-CA8C800B8683.png)

$$v_{*}(s) = \max_a  q_{*}(s, a)$$
$$q_{*}(s, a) = R_{s}^{a} + \gamma \sum_{s^{'} \in S} P_{ss^{'}}^{a} v_{*}(s^{'})$$
$$v_{*}(s) = \max_a R_s^a + \gamma \sum_{s^{'} \in S} P_{ss^{'}}^{a} v_{*}(s^{'})$$

贝尔曼最优等式是非线性的，不一定收敛，可以使用迭代的方法求其最优值：
* Value Iteration
* Policy Iteration
* Q-learning
* Sarsa


//TODO
